{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e84bb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTHONUNBUFFERED\"] = \"1\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/opt/openjdk@17\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/homebrew/Cellar/apache-spark/4.0.0/libexec\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"SPARK_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d868406e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/homebrew/opt/openjdk@17/bin:/opt/homebrew/Cellar/apache-spark/4.0.0/libexec/bin:/Users/aleksejkitajskij/Desktop/goit_spark/venv/bin:/opt/homebrew/opt/apache-spark/libexec/bin:/opt/anaconda3/bin:/opt/anaconda3/condabin:/Users/aleksejkitajskij/fam_terminal:/opt/homebrew/opt/openjdk@17/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/Library/Frameworks/Python.framework/Versions/3.12/bin:/Library/Frameworks/Python.framework/Versions/3.10/bin:/Library/Frameworks/Python.framework/Versions/3.11/bin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/usr/local/share/dotnet:~/.dotnet/tools'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "883284ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28b0aef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/25 18:34:34 WARN Utils: Your hostname, Aleksejs-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.226 instead (on interface en0)\n",
      "25/08/25 18:34:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/25 18:34:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Створення SparkSession\n",
    "spark = SparkSession.builder.appName(\"CSVStream\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a2b2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визначення схеми для CSV-файлу\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"event_time\", TimestampType(), True),\n",
    "    StructField(\"value\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4c3f01",
   "metadata": {},
   "source": [
    "# append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d82207e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/25 18:34:37 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/7x/q4zhf1wx08z6l2f4bqz5b8dc0000gn/T/temporary-ae70e0cf-9b7c-4f05-8c8a-1092c1d75441. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/08/25 18:34:37 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---+-------------------+-----+\n",
      "| id|         event_time|value|\n",
      "+---+-------------------+-----+\n",
      "|  1|2024-06-09 12:02:00|  cat|\n",
      "|  1|2024-06-09 12:02:00|  dog|\n",
      "+---+-------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+---+-------------------+-----+\n",
      "| id|         event_time|value|\n",
      "+---+-------------------+-----+\n",
      "|  1|2024-06-09 12:03:00|  dog|\n",
      "|  1|2024-06-09 12:03:00|  dog|\n",
      "+---+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Читання потокових даних із CSV-файлу\n",
    "csvDF = spark.readStream \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"data/data_for_streaming\")  # Папка для моніторингу нових файлів\n",
    "\n",
    "# Старт стримінгу й виведення результатів на екран\n",
    "query = (csvDF.writeStream\n",
    "         .trigger(availableNow=True)\n",
    "         # .trigger(processingTime='10 seconds')\n",
    "         .outputMode(\"append\")\n",
    "         .format(\"console\")\n",
    "         .start()\n",
    "         )\n",
    "\n",
    "# Чекаємо кінця роботи стримінгу\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d74265",
   "metadata": {},
   "source": [
    "# update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45bb3ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/25 18:43:10 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/7x/q4zhf1wx08z6l2f4bqz5b8dc0000gn/T/temporary-6831bc60-35c6-40bd-bed0-876e46aea0e6. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/08/25 18:43:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|value|count|\n",
      "+-----+-----+\n",
      "|  dog|    1|\n",
      "|  cat|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|value|count|\n",
      "+-----+-----+\n",
      "|  dog|    3|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvDF = spark.readStream \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"data/data_for_streaming\")\n",
    "\n",
    "# Групування даних за полем 'value' і підрахунок кількості\n",
    "countDF = csvDF.groupBy(\"value\").count()\n",
    "\n",
    "# Старт стримінгу та виведення результатів на екран\n",
    "query = (countDF.writeStream\n",
    "         .trigger(availableNow=True)\n",
    "         .outputMode(\"update\")\n",
    "         # .outputMode(\"update\")\n",
    "         .format(\"console\")\n",
    "         .start()\n",
    "         )\n",
    "\n",
    "# Чекаємо кінця роботи стримінгу\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0702a0",
   "metadata": {},
   "source": [
    "# complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6506de9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/25 18:43:21 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/7x/q4zhf1wx08z6l2f4bqz5b8dc0000gn/T/temporary-ed6914b5-857b-4095-adae-d20dc6f0ff94. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/08/25 18:43:21 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|value|count|\n",
      "+-----+-----+\n",
      "|  dog|    1|\n",
      "|  cat|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:=============================================>        (168 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|value|count|\n",
      "+-----+-----+\n",
      "|  dog|    3|\n",
      "|  cat|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "csvDF = spark.readStream \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"data/data_for_streaming\")\n",
    "\n",
    "# Групування даних за полем 'value' і підрахунок кількості\n",
    "countDF = csvDF.groupBy(\"value\").count()\n",
    "\n",
    "# Старт стримінгу та виведення результатів на екран\n",
    "query = (countDF.writeStream\n",
    "         .trigger(availableNow=True)\n",
    "         .outputMode(\"complete\")\n",
    "         # .outputMode(\"update\")\n",
    "         .format(\"console\")\n",
    "         .start()\n",
    "         )\n",
    "\n",
    "# Чекаємо кінця роботи стримінгу\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a98a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
